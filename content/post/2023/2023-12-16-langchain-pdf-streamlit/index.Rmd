---
title: '[Langchain] PDFë¥¼ í•™ìŠµí•œ ë‚˜ë§Œì˜ ì±—ë´‡ streamlitì— ë°°í¬ í•˜ê¸°'
author: ìµœì˜ìš©
date: '2023-12-16'
slug: langchain-pdf-streamlit
categories:
  - Langchain
tags:
  - streamlit
  - langchain
---

<center>
![](images/main1.PNG){width=80%}
</center>

ì´ë²ˆì—ëŠ” langchainì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ë§Œì˜ ì±—ë´‡ì„ ë§Œë“¤ê³ , streamlitì— ë°°í¬ í•´ë³´ì. ìµœê·¼ì— ì±…ì„ ì§‘í•„í•˜ê³ , ì±… ë‚´ìš© ëŒ€í•œ ì§ˆë¬¸ì„ ì¢…ì¢… ë°›ì•˜ì—ˆë‹¤. ì´ë•Œ ë¬¸ë“ ìƒê°ì´ ë‚¬ë‹¤.

<center>
ì´ë•Œ ì±—ë´‡ì´ ìˆìœ¼ë©´ ë„ì›€ì´ ë˜ê² ëŠ”ë°? <br>
langchainì„ ì´ì°¸ì— ì ‘ëª© ì‹œì¼œì…” streamlit ëŒ€ì‹œë³´ë“œì— ë„£ì–´ ë³´ì.
</center>

ê·¸ë˜ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ë§Œì˜ ì±—ë´‡ì„ ë§Œë“¤ì–´ ë³´ì•˜ë‹¤. ì£¼ì œëŠ” ì´ë²ˆì— ì¼ë˜ ì±…. 'ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ë°°ìš°ëŠ” íŒŒì´ì¬ ë¬¸ì œ í•´ê²° '

- [ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ë°°ìš°ëŠ” íŒŒì´ì¬ ë¬¸ì œ í•´ê²° ì±—ë´‡](https://apartment-board.streamlit.app/%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5%EC%B1%97%EB%B4%87)

<center>
![](images/main.PNG){width=80%}
</center>


ê·¸ëŸ¼ í•˜ë‚˜ì”© ë§Œë“¤ì–´ë³´ì. ìš°ì„  ë§Œë“¤ë©´ì„œ ì°¸ê³ ë¥¼ í•´ì•¼í•  ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 

- [ë„ì… ì‹œê¸‰! íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸° | ê¹€íƒœì˜](https://aifactory.space/task/2446/overview)
- [ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬ ìœ íŠœë¸Œ-íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ ë³´ê¸°](https://www.youtube.com/watch?v=2yv4PxE1Ks0)
- [Streamlit-Chat elements](https://docs.streamlit.io/library/api-reference/chat)

## Langchainì„ ì‚¬ìš©í•œ ë‚˜ë§Œì˜ ì±—ë´‡ ë§Œë“¤ê¸°

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

ìš°ì„  ì´ë²ˆì— í•„ìš”í•œ ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ í•´ì£¼ì.

- pypdf
  - íŒŒì´ì¬ìœ¼ë¡œ pdfë¥¼ ì½ì–´ ì˜¤ê¸° ìœ„í•¨ì´ë‹¤. ê·¸ëŸ¬ë‚˜, pdfì—ì„œë„ ì½ì„ìˆ˜ ì—†ëŠ” ì´ë¯¸ì§€í˜• í…ìŠ¤íŠ¸ëŠ” ë¶ˆê°€.(í…ŒìŠ¤íŠ¸ í•´ë³´ë ¤ë©´ ë§ˆìš°ìŠ¤ë¡œ ë“œë˜ê·¸ê°€ ê°€ëŠ¥í•˜ë©´ ëœë‹¤.)
- chromadb
  - ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° ë°ì´í„° ë² ì´ìŠ¤ì´ë‹¤. ìµœê·¼ì— ë¶€ìƒí•˜ê³  ìˆëŠ” ë²¡í„° DBë¥¼ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µí•œë‹¤. ë²¡í„° DBëŠ” ë‚˜ì¤‘ì— ê³µë¶€í• ë•Œ ìì„¸í•˜ê²Œ ë‹¤ë¤„ë³¼ ì˜ˆì •.
  - ì°¸ê³ ë¡œ í˜„ì‹œì (2023-12-17)ì—ì„œëŠ” íŒŒì´ì¬ 3.10ì´ ê°€ì¥ ì í•©í•˜ë‹¤. 
  - 3.11ì—ì„œëŠ” ì„¤ì¹˜ê°€ ì•ˆë˜ê³ , 3.9ì—ì„œëŠ” streamlitê³¼ì˜ í˜¸í™˜ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.
- tiktoken
  - íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬. pdfë¥¼ ì½ì–´ì˜¤ê³  í† í°í™” ì‹œì¼œì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.
- langchain
  - LLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬.
  - ê³µì‹ë¬¸ì„œì—ì„œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•˜ë©´ ì´ê³³ì—ì„œ ì—¬ëŸ¬ê°€ì§€ê³  ì°¸ê³  í•˜ë©´ ëœë‹¤.
  - ê³µì‹ë¬¸ì„œ: [www.langchain.com](https://www.langchain.com/)

```bash
pip install pypdf 
pip install chromadb
pip install tiktoken
pip install langchain
```

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

ìš°ì„  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ ì˜¤ì. ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒì˜ ìœ íŠœë¸Œë¥¼ ê¼­ ì°¸ê³  í•˜ì. 

- [ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬ ìœ íŠœë¸Œ-íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ ë³´ê¸°](https://www.youtube.com/watch?v=2yv4PxE1Ks0)

```python
import os

from langchain.chat_models import ChatOpenAI

from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cohere import CohereEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch
from langchain.vectorstores import Chroma

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
```

#### OpenAI API Key ë“±ë¡

ì´ì œ OpenAIì˜ APIí‚¤ë¥¼ ë„£ì–´ ì£¼ì.

```python
os.environ["OPENAI_API_KEY"] = 'OpenAI API í‚¤'
```

#### PDF ë¡œë“œ

ê·¸ë¦¬ê³  ë‚˜ì„œ ìš°ë¦¬ê°€ í•„ìš”í•œ pdfë¥¼ ì½ì–´ ì˜¨ë‹¤. pypdfì˜ PyPDFLoader()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½ì–´ ì˜¨ë‹¤.

```python
loader = PyPDFLoader('ìƒ˜í”Œ.pdf')
documents = loader.load()
```

#### ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë¶„í•  í•´ì£¼ì.

```python
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
```

ìš°ë¦¬ê°€ pdfì—ì„œ ì½ì–´ì˜¨ í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì‰½ê²Œ ì´í•´í•˜ê³  ì²˜ë¦¬ í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ ì´ë¥¼ ë²¡í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜ í•˜ì—¬ ì €ì¥ í•´ì£¼ì.

```python
embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(texts, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 2})
```

### í”„ë¡¬í”„íŠ¸ ê°œì„ 

í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„  í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì´ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ì˜ì–´ë¡œ ë‹µë³€ì„ í•˜ê²Œ ë˜ëŠ”ë°, system_templateì— í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ë‚´ìš©ì„ ì¶”ê°€ í•´ì„œ ë” ë§¤ë„ëŸ½ê²Œ í•œê¸€ë¡œ ê²°ê³¼ë¥¼ ë‚˜ì˜¤ê²Œ í•´ì¤€ë‹¤. ì´ ì—­ì‹œ ìì„¸í•œ ì‚¬í•­ì€ ìœ íŠœë¸Œë¥¼ ì°¸ê³  í•œë‹¤.

- [ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬ ìœ íŠœë¸Œ-íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ ë³´ê¸°](https://www.youtube.com/watch?v=2yv4PxE1Ks0)

```python
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

system_template="""
Use the following pieces of context to answer the users question shortly.
Given the following summaries of a long document and a question, create a final answer with references ("SOURCES"), use "SOURCES" in capital letters regardless of the number of sources.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
----------------
{summaries}

You MUST answer in Korean and in Markdown format:"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)
```

#### ChatGPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ

ChatGPTì˜ gpt-3.5-turboë¥¼ ì‚¬ìš©í•˜ì—¬ pdfë¥¼ í•™ìŠµ í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ chain()í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì¤€ë‹¤.

```python
chain_type_kwargs = {"prompt": prompt}

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)  # Modify model_name if you have access to GPT-4

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)
```

#### ê²°ê³¼ í™•ì¸

ì´ë ‡ê²Œ ì±…ì„ í•™ìŠµ ì‹œì¼œë³´ê³  ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì.

```python
query = "ì£¼ì œê°€ ë­ì•¼?"
result = chain(query)
result['answer']
```

```
ì£¼ì œëŠ” íŒŒì´ì¬ê³¼ AWSë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì²˜ë¦¬ ì…ë‹ˆë‹¤.
```

## Streamlit ì—°ë™

ì´ë²ˆì—ëŠ” ì´ë¥¼ Streamlitì— ì—°ë™ í•´ë³´ì. ì§€ë‚œë²ˆì— AWSì™€ íŒŒì´ì¬ ì±…ì„ ë§Œë“¤ë©´ì„œ Streamlit ëŒ€ì‹œë³´ë“œë¥¼ ê°™ì´ ìƒì„± í–ˆëŠ”ë°, ê±°ê¸°ì— ìƒˆë¡œìš´ í˜ì´ì§€ë¥¼ ì¶”ê°€ í•˜ê³  ê±°ê¸°ì— ì±—ë´‡ì„ ë„£ì„ ì˜ˆì •ì´ë‹¤. streamlitì—ì„œë„ chatbotì„ ìœ„í•œ ê¸°ëŠ¥ì„ ì¶”ê°€ í•´ì£¼ì—ˆë‹¤. Streamlitì˜ ì±—ë´‡ ê¸°ëŠ¥ì€ ë‹¤ìŒì˜ ê³µì‹ë¬¸ì„œë¥¼ ì°¸ê³  í•˜ì.

- [Streamlit-Chat elements](https://docs.streamlit.io/library/api-reference/chat)

### Streamlit ì½”ë“œ

streamlitì— ë“¤ì–´ê°€ëŠ” ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ë§¤ìš° í¡ì‚¬ í•˜ë‹¤. ë”°ë¼ì„œ, íŠ¹ì´ì‚¬í•­ê³¼ ì±—ë´‡ ì½”ë“œë§Œ ì„¤ëª… í•˜ê³  ì „ì²´ ì½”ë“œë¡œ ë§ˆë¬´ë¦¬ í•˜ë ¤ê³  í•œë‹¤. 

#### íŠ¹ì´ì‚¬í•­

Streamlitì„ githubì— ì—°ë™ í• ë•Œ chromadb ì„¤ì¹˜ê°€ ì˜ ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆë‹¤. ê·¸ëŸ´ë•Œ ë°°í¬í•˜ëŠ” íŒŒì´ì¬ì„ 3.10ìœ¼ë¡œ í•´ì£¼ê³  ë‹¤ìŒì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ í•´ì£¼ë©´ ëœë‹¤.

```python
import pysqlite3
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
import streamlit as st
import sqlite3
```

#### streamlit chatbot

ë‹¤ìŒì€ Stremalit ì „ìš© ì±—ë´‡ ì½”ë“œ ì´ë‹¤.

```python
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš” ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg =  generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)    
```

#### ì „ì²´ ì½”ë“œ 

ì´ì œ Streamlit ì— ë“¤ì–´ê°ˆ ì „ì²´ ì½”ë“œë¡œ ë§ˆë¬´ë¦¬

```python
import pysqlite3
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
import streamlit as st
import sqlite3


from langchain.llms import OpenAI

from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cohere import CohereEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch
from langchain.vectorstores import Chroma

import os

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain



st.set_page_config(
    page_title="ì§ˆì˜ì‘ë‹µì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


os.environ["OPENAI_API_KEY"] = 'OPENAI apikey' # í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

loader = PyPDFLoader('í•™ìŠµì— í•„ìš”í•œ pdf íŒŒì¼.pdf')
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(texts, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 2})
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

system_template="""Use the following pieces of context to answer the users question shortly.
Given the following summaries of a long document and a question, create a final answer with references ("SOURCES"), use "SOURCES" in capital letters regardless of the number of sources.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
----------------
{summaries}

You MUST answer in Korean and in Markdown format:"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)


chain_type_kwargs = {"prompt": prompt}

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)  # Modify model_name if you have access to GPT-4

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)


st.subheader('ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš”')


def generate_response(input_text):
  result = chain(input_text)
  return result['answer']

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš” ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg =  generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)    


```



## ì´í‰

ì´ë ‡ê²Œ langchainì„ ì‚¬ìš©í•˜ì—¬ pdfë¥¼ í•™ìŠµí•œ ë‚˜ë§Œì˜ ì±—ë´‡ì„ ë§Œë“¤ì–´ ë³´ì•˜ë‹¤. ê·¸ë¦¬ê³  Streamlitì— ë‚´ê°€ ì‘ì„±í•œ ì±…ì˜ pdfíŒŒì¼ì„ í•™ìŠµì‹œì¼œ ì±—ë´‡ì„ ë„£ì–´ ë´¤ë‹¤. ë­ì²´ì¸ê³¼ ë²¡í„° DBê°€ ìš”ìƒˆ ìì£¼ ë³´ì´ëŠ”ë°, ë‹¤ìŒì—ëŠ” ê·¸ê²ƒë„ ê±´ë“œë ¤ë´ì•¼ ê² ë‹¤.

---

## Reference

- [ë„ì… ì‹œê¸‰! íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸° | ê¹€íƒœì˜](https://aifactory.space/task/2446/overview)
- [ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬ ìœ íŠœë¸Œ-íšŒì‚¬ë‚´ê·œ ì±—ë´‡ ì§ì ‘ ë§Œë“¤ì–´ ë³´ê¸°](https://www.youtube.com/watch?v=2yv4PxE1Ks0)
- [ChatGPT AIì— ì†Œì„¤ì„ í•™ìŠµì‹œí‚¨ ë‹¤ìŒ ì§ˆë¬¸í•˜ê¸°](https://anpigon.tistory.com/389)
- [Streamlit LLM Gallery](https://streamlit.io/gallery?category=llms)