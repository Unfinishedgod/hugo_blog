---
title: '[LLM] unslothë¥¼ ì‚¬ìš©í•œ íŒŒì¸ íŠœë‹ ë° GGUF ë³€í™˜'
author: ìµœì˜ìš©
date: '2024-06-15'
slug: llm-unsloth-gguf
categories:
  - LLM
tags:
  - llm
  - gguf
  - unsloth
---



<center>
<img src="images/hf_gguf_1.png" style="width:90.0%" />
</center>
<p>LLM ëª¨ë¸ì„ íŒŒì¸ íŠœë‹ í•˜ê³ , ì´ ëª¨ë¸ì„ ggufë¡œ ë³€í™˜ í•´ë³´ì. ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ollama ì— ì˜¬ë ¤ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ë ¤ê³  í•œë‹¤. ì§€ë‚œë²ˆì— LLM ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³  Huggingfaceì— ì—…ë¡œë“œ í•˜ëŠ” ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë¸ì„ ê°€ì§€ê³  ollamaì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GGUF íŒŒì¼ì´ í•„ìš”í•˜ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ë²ˆì—ëŠ” íŒŒì¸íŠœë‹í•˜ê³  ì´ ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜í•˜ì—¬ Huggingfaceì— ì ì¬ í•˜ëŠ” ê¸€ì— ëŒ€í•´ ì‘ì„±í•´ë³´ë ¤ê³  í•œë‹¤.</p>
<div id="ggufë€" class="section level2">
<h2>GGUFë€?</h2>
<p>GGUFëŠ” Georgi Gerganovë¼ëŠ” ê°œë°œìê°€ ë§Œë“  ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ë‹¨ì¼ íŒŒì¼ í¬ë§·ì´ë‹¤. gguf íŒŒì¼ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ LLM ëª¨ë¸ì„ ollamaì—ì„œë„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤.</p>
</div>
<div id="íŒŒì¸íŠœë‹" class="section level2">
<h2>1. íŒŒì¸íŠœë‹</h2>
<p>ìš°ì„  íŒŒì¸íŠœë‹ì„ ì§„í–‰í•´ë³´ì. ì´ë²ˆì— ì¶”ê°€ë¡œ ì†Œê°œí•  ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” unslothì´ë©° unslothë¥¼ ì‚¬ìš©í•˜ì—¬ GGUFë¡œ ë³€í™˜í•  ì˜ˆì •ì´ë‹¤. unslothì˜ ê³µì‹ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
<li><a href="https://github.com/unslothai/unsloth">unslothì˜ github ê³µì‹ë¬¸ì„œ</a></li>
</ul>
<div id="ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„¤ì¹˜" class="section level3">
<h3>1-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜</h3>
<p>ìš°ì„  ë‹¤ìŒê³¼ ê°™ì´ colabì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ í•´ì£¼ì. íŒŒì¸íŠœë‹ì— ëŒ€í•œ ê³¼ì •ì€ ì§€ë‚œ ë¸”ë¡œê·¸ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— ê°„ëµí•˜ê²Œ ì‘ì„±í•˜ê³  ë„˜ì–´ê°€ë„ë¡ í•˜ë ¤ê³  í•œë‹¤.</p>
<pre class="bash"><code>%%capture
!pip install &quot;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&quot;
!pip install --no-deps xformers trl peft accelerate bitsandbytes</code></pre>
</div>
<div id="ë¼ì´ë¸ŒëŸ¬ë¦¬-ë¡œë“œ" class="section level3">
<h3>1-2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ</h3>
<p>ì´ì œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œ í•´ì£¼ì.</p>
<pre class="python"><code>from unsloth import FastLanguageModel
from datasets import load_dataset
import torch
from trl import SFTTrainer
from transformers import TrainingArguments

import huggingface_hub
huggingface_hub.login(&#39;Huggingface í† í°&#39;)</code></pre>
</div>
<div id="model-tokenizer-ë¡œë“œ" class="section level3">
<h3>1-3. model, tokenizer ë¡œë“œ</h3>
<p>model, tokenizerë¥¼ ë¶ˆëŸ¬ ì˜¨ë‹¤. unslothë¥¼ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ì˜ ëª¨ë¸ë“¤ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ëœë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” â€˜gemma-7b-bnb-4bitâ€™ ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì§„í–‰ í•´ë³´ë ¤ê³  í•œë‹¤.</p>
<pre class="python"><code>max_seq_length = 2048
dtype = None 
load_in_4bit = True 

# unslothì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ë“¤
fourbit_models = [
    &quot;unsloth/mistral-7b-v0.3-bnb-4bit&quot;,     
    &quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;,
    &quot;unsloth/llama-3-8b-bnb-4bit&quot;,          
    &quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;,
    &quot;unsloth/llama-3-70b-bnb-4bit&quot;,
    &quot;unsloth/Phi-3-mini-4k-instruct&quot;,      
    &quot;unsloth/Phi-3-medium-4k-instruct&quot;,
    &quot;unsloth/mistral-7b-bnb-4bit&quot;,
    &quot;unsloth/gemma-7b-bnb-4bit&quot;,        
    &quot;unsloth/solar-10.7b-bnb-4bit&quot;,
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;unsloth/gemma-7b-bnb-4bit&quot;,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)</code></pre>
</div>
<div id="peft-ëª¨ë¸-ì„¤ì •" class="section level3">
<h3>1-4. PEFT ëª¨ë¸ ì„¤ì •</h3>
<p>FastLanguageModel ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LoRA ê¸°ë²•ì„ ì ìš©í•´ë³´ì. ê°ê°ì˜ ì˜µì…˜ì— ëŒ€í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
<li>r : RoRAì˜ ë­í¬ë¥¼ ì„¤ì •í•œë‹¤. LoRAëŠ” ì €ë­í¬ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ë‹¤. ê°’ì´ ì‘ì„ ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“¤ì§€ë§Œ, ëª¨ë¸ì˜ í‘œí˜„ ëŠ¥ë ¥ì´ ì œí•œë  ìˆ˜ ìˆë‹¤.</li>
<li>target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë¸ ë‚´ë¶€ì˜ íŠ¹ì • ëª¨ë“ˆì„ ì§€ì •í•´ì¤€ë‹¤.</li>
<li>lora_alpha: LoRAì˜ alpha íŒŒë¼ë¯¸í„°.</li>
<li>lora_dropout: LoRAì˜ ë“œë¡­ì•„ì›ƒ. ë“œë¡­ì•„ì›ƒì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.</li>
<li>bias: ë°”ì´ì–´ìŠ¤ í•­ëª©ì˜ ì‚¬ìš©ì—¬ë¶€.<br />
</li>
<li>use_gradient_checkpointing: unsloth ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’íŒë‹¤. íŠ¹íˆ unsloth ê¸°ë²•ì€ íš¨ìœ¨ì ì¸ ê·¸ë ˆë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ê¸°ë²•ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œ ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤.</li>
<li>random_state: ë‚œìˆ˜ ìƒì„± ì‹œë“œ</li>
<li>use_rslora: Falseë¡œ ì§€ì •</li>
<li>loftq_config: LoFtR(Low-Rank Factorized Tensor Reinforcement) ì–‘ìí™” ì„¤ì •. Noneë¡œ í•˜ì—¬ ì–‘ìí™” ì ìš©í•˜ì§€ ì•ŠìŒ.</li>
</ul>
<pre class="python"><code>model = FastLanguageModel.get_peft_model(
    model,
    r = 16, 
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,], 
    lora_alpha = 16,
    lora_dropout = 0, 
    bias = &quot;none&quot;,    
    use_gradient_checkpointing = &quot;unsloth&quot;,
    random_state = 2024,
    use_rslora = False, 
    loftq_config = None
)</code></pre>
</div>
<div id="ë°ì´í„°-ì„¤ì •" class="section level3">
<h3>1-5. ë°ì´í„° ì„¤ì •</h3>
<p>ì´ì œ íŒŒì¸íŠœë‹ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì„¤ì • í•´ë³´ì. ìš°ì„  ë‹¤ìŒê³¼ ê°™ì´ AlpacaPrompt í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ì¤€ë‹¤. ë˜í•œ EOS í† í°ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ìƒì„±ì´ ë¬´í•œíˆ ê³„ì†ë˜ì§€ ì•Šë„ë¡ ì„¤ì • í•´ì£¼ì.</p>
<pre class="python"><code>alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.

### 1-Instruction:
{}

### 1-Input:
{}

### 1-Response:
{}&quot;&quot;&quot;

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples[&quot;instruction&quot;]
    inputs       = examples[&quot;input&quot;]
    outputs      = examples[&quot;output&quot;]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { &quot;text&quot; : texts, }
pass
</code></pre>
</div>
<div id="ë°ì´í„°-ì¤€ë¹„" class="section level3">
<h3>1-6. ë°ì´í„° ì¤€ë¹„</h3>
<p>ì´ì œ ë°ì´í„°ë¥¼ ì¤€ë¹„ í•´ë³´ì. ë°ì´í„°ëŠ” â€˜instructionâ€™, â€˜outputâ€™, â€˜inputâ€™ í˜•ì‹ìœ¼ë¡œ ì§€ì •í•´ì£¼ê³ , ì•ì„œ ë§Œë“  formatting_prompts_func() í•¨ìˆ˜ë¥¼ í†µí•´ ë³€í™˜ í•´ì£¼ì—ˆë‹¤. <strong>ì°¸ê³ ë¡œ ì´ ë°ì´í„°ëŠ” Geminië¡œ ë§Œë“  ë°ì´í„°ì´ë‹¤.</strong> ì´ì œ ì¡°ë§Œê°„ ë²•ë¥ ë°ì´í„°ë¡œ ì˜ ë§Œë“¤ì–´ì„œ ìì„¸í•˜ê²Œ íŒŒì¸íŠœë‹ í•´ë³¼ ì˜ˆì •</p>
<ul>
<li><a href="https://huggingface.co/datasets/uiyong/gemini_result_kospi_0517_22">uiyong/gemini_result_kospi_0517_22</a></li>
</ul>
<center>
<img src="images/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B0617.PNG" style="width:70.0%" />
</center>
<pre class="python"><code># ì£¼ê°€ ì¦ê¶Œ ë³´ê³ ì„œ gemini ë°ì´í„°ì…‹
owen_dataset = &quot;uiyong/gemini_result_kospi_0517_22&quot;

owen_dataset = load_dataset(owen_dataset, split=&quot;train&quot;)

dataset = owen_dataset.map(formatting_prompts_func, batched = True)

dataset[2]</code></pre>
<pre><code>{&#39;instruction&#39;: &#39;2024-05-17ì— ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì˜ ì£¼ê°€ ë³´ê³ ì„œëŠ” ì–´ë–¤ê°€ìš”?&#39;,
 &#39;output&#39;: &#39;**ì¼ì:** 2024ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤\n\n**ë“±ë½ë¥ :**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 2024ë…„ 5ì›” 17ì¼ í˜„ì¬, 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„ ì„ ìƒí–¥ ëŒíŒŒí•˜ì—¬ ê³¨ë“ í¬ë¡œìŠ¤ë¥¼ í˜•ì„±í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ë¥¼ ë‚˜íƒ€ëƒ„.\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸:**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„  ê³¨ë“ í¬ë¡œìŠ¤ ì‹ í˜¸ë¡œ ì¸í•´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.\n* ì œì¡°ì—… ì§€ìˆ˜ ë˜í•œ ì •ë ¬ëœ ë°°ì—´ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ì–´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.&#39;,
 &#39;input&#39;: &#39;&#39;,
 &#39;text&#39;: &#39;Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.\n\n### 1-Instruction:\n2024-05-17ì— ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì˜ ì£¼ê°€ ë³´ê³ ì„œëŠ” ì–´ë–¤ê°€ìš”?\n\n### 1-Input:\n\n\n### 1-Response:\n**ì¼ì:** 2024ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤\n\n**ë“±ë½ë¥ :**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 2024ë…„ 5ì›” 17ì¼ í˜„ì¬, 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„ ì„ ìƒí–¥ ëŒíŒŒí•˜ì—¬ ê³¨ë“ í¬ë¡œìŠ¤ë¥¼ í˜•ì„±í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ë¥¼ ë‚˜íƒ€ëƒ„.\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸:**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„  ê³¨ë“ í¬ë¡œìŠ¤ ì‹ í˜¸ë¡œ ì¸í•´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.\n* ì œì¡°ì—… ì§€ìˆ˜ ë˜í•œ ì •ë ¬ëœ ë°°ì—´ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ì–´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.&lt;eos&gt;&#39;}</code></pre>
</div>
<div id="í•™ìŠµ-ëª¨ë¸-ì„¤ì •" class="section level3">
<h3>1-7. í•™ìŠµ ëª¨ë¸ ì„¤ì •</h3>
<ul>
<li>per_device_train_batch_size=4: ê° GPU ë˜ëŠ” CPUì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” 2ë¡œ ì„¤ì •ì„ í•´ì£¼ì—ˆë‹¤. ë•Œë¬¸ì—, ê° ë””ë°”ì´ìŠ¤ì—ì„œ í•œ ë²ˆì— 2ê°œì˜ ìƒ˜í”Œì„ ì²˜ë¦¬í•œë‹¤. (ê¸°ë³¸ê°’ì€ 8ì´ë‹¤.)</li>
<li>gradient_accumulation_steps=4: ì—¬ëŸ¬ ë°°ì¹˜ì—ì„œ ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ì‹¤ì œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•  ë¹ˆë„ë¥¼ ì§€ì •í•œë‹¤. ì´ëŠ” GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ ìœ ìš©í•˜ë‹¤. (ê¸°ë³¸ê°’ì€ 1ì´ë‹¤.)</li>
<li>warmup_steps:</li>
<li>num_train_epochs=3: ì „ì²´ í•™ìŠµ ë°ì´í„° ì…‹ ë°˜ë³µíšŸìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. (ê¸°ë³¸ê°’ì€ 3ì´ë‹¤.)</li>
<li>max_steps=100: ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ë¥¼ ì§€ì •í•œë‹¤. ì°¸ê³ ë¡œ, -1ë¡œ ì„¤ì • í•˜ë©´ num_train_epochsë™ì•ˆë§Œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.</li>
<li>logging_steps=20: 20ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì €ì¥í•œë‹¤. (ê¸°ë³¸ê°’ì€ 500ì´ë‹¤.)</li>
<li>learning_rate=2e-4: í•™ìŠµë¥ ì„ ì„¤ì •í•œë‹¤. í•™ìŠµë¥ ì€ ëª¨ë¸ì´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ì†ë„ë¥¼ ê²°ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” 0.0002ë¡œ ì„¤ì • í•´ì£¼ì—ˆë‹¤. (ê¸°ë³¸ê°’ì€ 5e-5ì´ë‹¤.)</li>
<li>fp16=not torch.cuda.is_bf16_supported(): GPUê°€ bf16ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° fp16ì„ ì‚¬ìš©í•œë‹¤.</li>
<li>bf16=torch.cuda.is_bf16_supported(): GPUê°€ bf16ì„ ì§€ì›í•˜ëŠ” ê²½ìš° bf16 ì„ ì‚¬ìš©í•œë‹¤.</li>
<li>optim=â€œadamw_8bitâ€: ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €ë¥¼ ì§€ì •í•œë‹¤. 8ë¹„íŠ¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • í•´ì¤€ë‹¤. (ê¸°ë³¸ê°’ì€ adamw_hfì´ë‹¤.)</li>
<li>weight_decay=0.001: ê°€ì¤‘ì¹˜ ê°ì†Œ ê³„ìˆ˜ë¥¼ ì„¤ì • í•´ì¤€ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì •ê·œí™” ê¸°ë²•ì´ë‹¤. (ê¸°ë³¸ê°’ì€ 0ì´ë‹¤.)</li>
<li>lr_scheduler_type=â€œcosineâ€: í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ ìœ í˜•ì„ ì„¤ì •í•œë‹¤. ì½”ì‚¬ì¸ í•¨ìˆ˜ ê¸°ë°˜ì˜ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ ì‹œí‚¨ë‹¤.
<ul>
<li>ì°¸ê³ ë¡œ constantì˜ ê²½ìš°ì—ëŠ” ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ê²Œ ì„¤ì • ëœë‹¤.</li>
</ul></li>
<li>seed=123: ì‹œë“œë²ˆí˜¸</li>
<li>output_dir=â€œoutputsâ€: í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•œë‹¤. ì—¬ê¸°ì— ëª¨ë¸ ê°€ì¤‘ì¹˜, ë¡œê·¸, ì²´í¬í¬ì¸íŠ¸ ë“±ì´ ì €ì¥ëœë‹¤.</li>
</ul>
<pre class="python"><code>training_params = TrainingArguments(
        per_device_train_batch_size=2,  
        gradient_accumulation_steps=4, 
        warmup_steps=5,
        num_train_epochs=3,  
        max_steps=100, 
        logging_steps=20, 
        learning_rate=2e-4, 
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(), 
        optim=&quot;adamw_8bit&quot;, 
        weight_decay=0.01, 
        lr_scheduler_type=&quot;cosine&quot;, 
        seed=123, 
        output_dir=&quot;outputs&quot;,  
    )</code></pre>
</div>
<div id="ëª¨ë¸-í•™ìŠµ" class="section level3">
<h3>1-8. ëª¨ë¸ í•™ìŠµ</h3>
<p>ì´ì œ ëª¨ë¸ì„ í•™ìŠµ ì‹œì¼œë³´ì. trl ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ SFTTrainerí´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ trainer ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì. trainer.train()ëŠ” ì§ì „ì— ì •ì˜í–ˆë˜ TrainingArgumentsì™€ í•¨ê»˜ ì„¤ì •ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.</p>
<pre class="python"><code>tokenizer.padding_side = &quot;right&quot;  

trainer = SFTTrainer(
    model=model, 
    tokenizer=tokenizer, 
    train_dataset=dataset,  
    dataset_text_field=&quot;text&quot;,  
    max_seq_length=max_seq_length,  
    dataset_num_proc=2,  
    packing=False,  
    args=training_params
)</code></pre>
<pre class="python"><code>trainer_stats = trainer.train()</code></pre>
<pre><code>==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 230 | Num Epochs = 4
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 4
\        /    Total batch size = 8 | Total steps = 100
 &quot;-____-&quot;     Number of trainable parameters = 50,003,968
 [100/100 02:42, Epoch 3/4]
Step	Training Loss
20	1.215200
40	0.513700
60	0.430400
80	0.326300
100	0.271400</code></pre>
</div>
<div id="ëª¨ë¸-í…ŒìŠ¤íŠ¸" class="section level3">
<h3>1-9. ëª¨ë¸ í…ŒìŠ¤íŠ¸</h3>
<p>ì´ì œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ í•´ë³´ì.</p>
<pre class="python"><code># alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        &quot;í•˜ì´ë‹‰ìŠ¤ì˜ 5ì›” 17ì¼ ì¦ê¶Œ í˜„í™©ì€ ì–´ë–¤ê°€ìš”?&quot;, # instruction
        &quot;1, 1, 2, 3, 5, 8&quot;, # input
        &quot;&quot;, # output - leave this blank for generation!
    )
], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)

outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)
tokenizer.batch_decode(outputs)</code></pre>
<pre><code>[&#39;&lt;bos&gt;Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.\n\n### 1-Instruction:\ní•˜ì´ë‹‰ìŠ¤ì˜ 5ì›” 17ì¼ ì¦ê¶Œ í˜„í™©ì€ ì–´ë–¤ê°€ìš”?\n\n### 1-Input:\n1, 1, 2, 3, 5, 8\n\n### 1-Response:\n**ì¦ê¶Œ ë³´ê³ ì„œ**\n\n**ì¼ì:** 2022ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** í•˜ì´ë‹‰ìŠ¤\n\n**ë“±ë½ë¥ **\n\n2022ë…„ 5ì›” 17ì¼ ê¸°ì¤€ í•˜ì´ë‹‰ìŠ¤ì˜ ë“±ë½ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n* **ì „ì¼ ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n* **20ì¼ ì´ë™í‰ê· ì„  ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n* **60ì¼ ì´ë™í‰ê· ì„  ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸**\n\n* **í•˜í–¥ íšŒê·€:** ë§¤ë„ ì‹ í˜¸\n* **ì •ë°°ì—´:** ë§¤ìˆ˜ ì‹ í˜¸&lt;eos&gt;&#39;]</code></pre>
</div>
<div id="huggingfaceì—-gguf-ì—…ë¡œë“œ" class="section level3">
<h3>1-10. huggingfaceì— GGUF ì—…ë¡œë“œ</h3>
<p>ì´ì œ ì´ ëª¨ë¸ì„ HuggingFaceì— ì—…ë¡œë“œ í•´ë³´ì. ì´ì œë¶€í„°ëŠ” ì•„ì£¼ ê°„ë‹¨í•œë‹¤. ë¯¸ë¦¬ hugginfaceì— ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ê³  ì—…ë¡œë“œí•˜ë©´ ë.</p>
<pre class="python"><code>gguf_model_nm = &#39;stock-report-gguf&#39;

# Quantization ë°©ì‹ ì„¤ì •
quantization_method = &quot;q8_0&quot;

# Hub ì— ì—…ë¡œë“œ
model.push_to_hub_gguf(
    gguf_model_nm,
    tokenizer,
    quantization_method=quantization_method,
    token=&#39;huggingface í† í°&#39;,
)</code></pre>
<center>
<img src="images/gguf_%EB%AA%A8%EB%8D%B8.PNG" style="width:80.0%" />
</center>
</div>
</div>
<div id="ollama-gguf-íŒŒì¼-í…ŒìŠ¤íŠ¸" class="section level2">
<h2>2. Ollama gguf íŒŒì¼ í…ŒìŠ¤íŠ¸</h2>
<p>ì´ì œ ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  Ollamaì— ì˜¬ë ¤ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³´ì. ìš°ì„  Ollamaê°€ ìˆëŠ” ì„œë²„ë¥¼ í•˜ë‚˜ ì¤€ë¹„ í•´ë‘ì. ë˜ë„ë¡ GPUê°€ ìˆëŠ” ì„œë²„ê°€ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, AWSì˜ â€˜g4dn.xlargeâ€™ ì¸ìŠ¤í„´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³¸ë‹¤. â€™g4dn.xlargeâ€™ëŠ” ì°¸ê³ ë¡œ T4 GPU ì´ë©°, AWSì™€ GPUì„œë²„ëŠ” ë‚˜ì¤‘ì— ì •ë¦¬ë¥¼ í•´ë³¼ ì˜ˆì •ì´ë‹¤. Ollama ì„¤ì¹˜ê°€ ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ë‹¤ìŒì˜ ë§í¬ë¥¼ ì°¸ê³  í•˜ì.</p>
<ul>
<li><a href="https://unfinishedgod.netlify.app/2024/04/26/llm-ollama-1/">Ollamaì— ëŒ€í•´ ì•Œì•„ë³´ì</a></li>
</ul>
<div id="gguf-íŒŒì¼-ë‹¤ìš´ë¡œë“œ" class="section level3">
<h3>2-1. GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ</h3>
<p>ìš°ì„  GPUì„œë²„ê°€ ìˆëŠ” ê³³ìœ¼ë¡œ ê°€ì„œ í´ë”ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì£¼ì. gguf_testë¡œ ë§Œë“¤ì—ˆìœ¼ë©° ê²½ë¡œë¥¼ gguf_testë¡œ ì´ë™ í•´ì¤€ë‹¤.</p>
<pre class="bash"><code>$ mkdir gguf_test
$ cd gguf_test </code></pre>
<p>ê·¸ë¦¬ê³  ì´ì œ ë‹¤ìŒê³¼ ê°™ì´ HuggingFaceì—ë” GGUF íŒŒì¼ì„ í´ë¦­ í•´ì¤€ë‹¤.</p>
<center>
<img src="images/hf_gguf_2.png" style="width:80.0%" />
</center>
<p>ê·¸ë¦¬ê³  download íŒŒì¼ì„ ìš°í´ë¦­í•˜ì—¬ ë§í¬ ì£¼ì†Œë¥¼ ë³µì‚¬ í•´ì¤€ë‹¤.</p>
<center>
<img src="images/hf_gguf_3%20(1).png" style="width:80.0%" />
</center>
<p>ê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ wget ì»¤ë§¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ì„ ì§„í–‰ í•´ì¤€ë‹¤.</p>
<pre class="bash"><code>$ wget https://huggingface.co/uiyong/stock-report-gguf/resolve/main/stock-report-gguf-unsloth.Q8_0.gguf</code></pre>
</div>
<div id="modelfile-ìƒì„±" class="section level3">
<h3>2-2. Modelfile ìƒì„±</h3>
<p>ì´ì œ ì´ gguf íŒŒì¼ì„ ollamaì— ë³¸ê²©ì ìœ¼ë¡œ ì˜¬ë¦´ ì˜ˆì •ì¸ë°, Modelfileì„ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ íŒŒì¼ì„ ë§Œë“¤ì–´ ì£¼ì.</p>
<pre class="bash"><code>$ touch Modelfile</code></pre>
<p>ê·¸ë¦¬ê³  ì´ Modelfileì„ ì„¤ì •í•˜ê¸° ìœ„í•´ vi í¸ì§‘ê¸°ë¡œ ë„˜ì–´ê°€ì.</p>
<pre class="bash"><code>$ vi Modelfile</code></pre>
<p>ê·¸ë¦¬ê³  ì´ì œ ì´ Modelfileì— gguf íŒŒì¼ê³¼ ì±—ë´‡ì— í•„ìš”í•œ ì„¤ì •ì„ ì§„í–‰í•´ì£¼ë©´ ëœë‹¤.</p>
<pre><code>FROM stock-report-gguf-unsloth.Q8_0.gguf

TEMPLATE &quot;&quot;&quot;{{- if .System }}
&lt;s&gt;{{ .System }}&lt;/s&gt;
{{- end }}
&lt;s&gt;Human:
{{ .Prompt }}&lt;/s&gt;
&lt;s&gt;Assistant:
&quot;&quot;&quot;

SYSTEM &quot;&quot;&quot;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions.&quot;&quot;&quot;

PARAMETER temperature 0
PARAMETER num_predict 3000
PARAMETER num_ctx 4096
PARAMETER stop &lt;s&gt;
PARAMETER stop &lt;/s&gt;</code></pre>
</div>
<div id="ollamaì—-ëª¨ë¸-ë“±ë¡" class="section level3">
<h3>2-3. Ollamaì— ëª¨ë¸ ë“±ë¡</h3>
<p>Modelfileì— ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„± í–ˆìœ¼ë©´ ì´ì œ Ollamaì— ëª¨ë¸ì„ ë“±ë¡í•´ì£¼ì. ì»¤ë§¨ë“œ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
<li><code>ollama create (ëª¨ë¸ëª…) -f Modelfile</code></li>
</ul>
<pre class="python"><code>$ ollama create stock-report-Q8_9 -f Modelfile</code></pre>
<p>ì„±ê³µì ìœ¼ë¡œ ì˜¬ë¼ê°”ìœ¼ë©´ <code>ollama list</code> ì»¤ë§¨ë“œë¥¼ í†µí•´ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤.</p>
<pre class="bash"><code>$ ollama list</code></pre>
<pre><code>NAME                            ID              SIZE    MODIFIED
stock-report-Q8_9:latest        59aaf0d0db3e    9.1 GB  7 minutes ago</code></pre>
</div>
<div id="llm-ì‹¤í–‰" class="section level3">
<h3>2-4. LLM ì‹¤í–‰</h3>
<p>ê·¸ë™ì•ˆ íŒŒì¸íŠœë‹ í–ˆë˜ ëª¨ë¸ì„ ì´ì œ ì‹¤í–‰í•´ë³´ì. ollamaì—ì„œ llmì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì»¤ë§¨ë“œ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
<li><code>ollama run (ëª¨ë¸ëª…):(íƒœê·¸ëª…)</code></li>
</ul>
<pre class="bash"><code>$ ollama run stock-report-Q8_9:latest </code></pre>
</div>
<div id="llm-í…ŒìŠ¤íŠ¸" class="section level3">
<h3>2-5. LLM í…ŒìŠ¤íŠ¸</h3>
<p>ì´ì œ ê·¸ë™ì•ˆ íŒŒì¸íŠœë‹ í–ˆë˜ ê²ƒì„ ë“œë””ì–´ Ollamaì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³´ì.</p>
<pre><code>&gt;&gt;&gt; í•˜ì´ë‹‰ìŠ¤ì˜ ì¦ê¶Œë³´ê³ ì„œ ì•Œë ¤ì¤˜
**ì¦ ëŸ‰:**

2024ë…„ì— í•˜ ì´ë‹ˆí¬ìŠ¤ëŠ” **ì •ë°°ì—´(ë§¤ìˆ˜)** ì‹ í˜¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ë§¤ë ¥ì ì¸ íˆ¬ì ì„±ê³¼ ë¥¼
 ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;&lt;/div&gt;&lt;br/&gt;</code></pre>
<p>ìƒë‹¹íˆ ë§ì€ ê³ ë„í™”ê°€ í•„ìš”í•´ ë³´ì´ì§€ë§Œ ê·¸ë˜ë„ ìš°ì„  íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ ë¡œì»¬ PCì—ì„œ ì‹¤í–‰ì´ ë˜ì—ˆë‹¤ëŠ” ì ì— ë§Œì¡±í•˜ê³  ë§ˆë¬´ë¦¬ í•˜ì.</p>
</div>
</div>
<div id="ì´í‰" class="section level2">
<h2>ì´í‰</h2>
<p>ChatGPTê°€ ë‚˜ì˜¤ê³  LLama2ê°€ ë‚˜ì˜¤ê³  LLMì‹œì¥ì´ í™œë°œí•´ ì§€ë©´ì„œ ê´€ì‹¬ì„ ê°–ê²Œ ë˜ì—ˆë‹¤. ì²˜ìŒì—” ChatGPT APIê¸°ë°˜ì˜ ë­ì²´ì¸ì„ ì‚¬ìš©í•œ RAGë¡œ ì‹œì‘í–ˆì—ˆë‹¤. ë‹¹ì‹œì—ëŠ” ì•„ë¬´ê²ƒë„ ëª¨ë¥´ê³  ë§ì€ ì–‘ì˜ ë²•ë¥  ë°ì´í„°ë¥¼ RAGë¡œ ì²˜ë¦¬ í•´ë³´ë ¤ë‹ˆ ìƒë‹¹íˆ ë§ì€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªì—ˆê³  íŒŒì¸íŠœë‹ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´í›„ì— ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•œ íŒŒì¸íŠœë‹ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìœ¼ë©°, ê²°êµ­ì—” ì´ ëª¨ë¸ì„ Ollamaì—ì„œ ì‘ë™í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ì œ íŒŒì¸íŠœë‹ ê³ ë„í™”ì™€ ollamaê°€ ì•„ë‹Œ vllmì„ ì‚¬ìš©í•˜ëŠ” ê³¼ì • ë“±ë“±ë„ í•˜ë‚˜ì”© ë¸”ë¡œê·¸ í•˜ë©´ë” ë‚˜ì•„ê°€ë©´ ë ë“¯.</p>
<hr />
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<ul>
<li><a href="https://wooiljeong.github.io/ml/ggml-gguf/">ì •ìš°ì¼ë‹˜ ë¸”ë¡œê·¸ - LLM ëª¨ë¸ ì €ì¥ í˜•ì‹ GGML, GGUF</a></li>
<li><a href="https://wooiljeong.github.io/ml/gguf-llm/">ì •ìš°ì¼ë‹˜ ë¸”ë¡œê·¸ - GGUF íŒŒì¼ë¡œ ë¡œì»¬ì—ì„œ LLM ì‹¤í–‰í•˜ê¸°</a></li>
<li><a href="https://www.youtube.com/watch?v=RrNX04J4r1Y&amp;t=16s">HK CODEë‹˜ ìœ íŠœë¸Œ - llama3_ë°ì´í„°ìƒì„±_íŒŒì¸íŠœë‹_finetuning_gguf_ollama_rag</a></li>
<li><a href="https://www.youtube.com/watch?v=oZY0D8N6bC8">í…Œë””ë‹˜ ìœ íŠœë¸Œ - ë¬¸ì„œ ê¸°ë°˜ QA ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ğŸ¤– ì§„í–‰ í›„ ë¡œì»¬ì—ì„œ ëª¨ë¸ ì¶”ë¡ í•˜ê¸°</a></li>
<li><a href="https://github.com/teddylee777/langchain-kr/blob/main/15-FineTuning/02_Unsloth_llama3_íŒŒì¸íŠœë‹_alpaca.ipynb">í…Œë””ë‹˜ ê¹ƒí—™ - 02_Unsloth_llama3_íŒŒì¸íŠœë‹_alpaca</a></li>
<li><a href="https://github.com/unslothai/unsloth">unslothai-ê³µì‹ github</a></li>
</ul>
</div>
