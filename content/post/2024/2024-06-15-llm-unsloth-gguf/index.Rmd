---
title: '[LLM] unslothë¥¼ ì‚¬ìš©í•œ íŒŒì¸ íŠœë‹ ë° GGUF ë³€í™˜'
author: ìµœì˜ìš©
date: '2024-06-15'
slug: llm-unsloth-gguf
categories:
  - LLM
tags:
  - llm
  - gguf
  - unsloth
---

<center>
![](images/hf_gguf_1.png){width=90%}
</center>



LLM ëª¨ë¸ì„ íŒŒì¸ íŠœë‹ í•˜ê³ , ì´ ëª¨ë¸ì„ ggufë¡œ ë³€í™˜ í•´ë³´ì. ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ollama ì— ì˜¬ë ¤ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ë ¤ê³  í•œë‹¤. ì§€ë‚œë²ˆì— LLM ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³  Huggingfaceì— ì—…ë¡œë“œ í•˜ëŠ” ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë¸ì„ ê°€ì§€ê³  ollamaì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GGUF íŒŒì¼ì´ í•„ìš”í•˜ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ë²ˆì—ëŠ” íŒŒì¸íŠœë‹í•˜ê³  ì´ ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜í•˜ì—¬ Huggingfaceì— ì ì¬ í•˜ëŠ” ê¸€ì— ëŒ€í•´ ì‘ì„±í•´ë³´ë ¤ê³  í•œë‹¤.

## GGUFë€?

GGUFëŠ” Georgi Gerganovë¼ëŠ” ê°œë°œìê°€ ë§Œë“  ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ë‹¨ì¼ íŒŒì¼ í¬ë§·ì´ë‹¤. gguf íŒŒì¼ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ LLM ëª¨ë¸ì„ ollamaì—ì„œë„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤. 

## 1. íŒŒì¸íŠœë‹

ìš°ì„  íŒŒì¸íŠœë‹ì„ ì§„í–‰í•´ë³´ì. ì´ë²ˆì— ì¶”ê°€ë¡œ ì†Œê°œí•  ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” unslothì´ë©° unslothë¥¼ ì‚¬ìš©í•˜ì—¬ GGUFë¡œ ë³€í™˜í•  ì˜ˆì •ì´ë‹¤. unslothì˜ ê³µì‹ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- [unslothì˜ github ê³µì‹ë¬¸ì„œ](https://github.com/unslothai/unsloth)

### 1-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

ìš°ì„  ë‹¤ìŒê³¼ ê°™ì´ colabì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ í•´ì£¼ì. íŒŒì¸íŠœë‹ì— ëŒ€í•œ ê³¼ì •ì€ ì§€ë‚œ ë¸”ë¡œê·¸ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— ê°„ëµí•˜ê²Œ ì‘ì„±í•˜ê³  ë„˜ì–´ê°€ë„ë¡ í•˜ë ¤ê³  í•œë‹¤.

```bash
%%capture
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers trl peft accelerate bitsandbytes
```

### 1-2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

ì´ì œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œ í•´ì£¼ì. 

```python
from unsloth import FastLanguageModel
from datasets import load_dataset
import torch
from trl import SFTTrainer
from transformers import TrainingArguments

import huggingface_hub
huggingface_hub.login('Huggingface í† í°')
```

### 1-3. model, tokenizer ë¡œë“œ

model, tokenizerë¥¼ ë¶ˆëŸ¬ ì˜¨ë‹¤. unslothë¥¼ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ì˜ ëª¨ë¸ë“¤ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ëœë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” 'gemma-7b-bnb-4bit' ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì§„í–‰ í•´ë³´ë ¤ê³  í•œë‹¤.

```python
max_seq_length = 2048
dtype = None 
load_in_4bit = True 

# unslothì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ë“¤
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",     
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",          
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",      
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",        
    "unsloth/solar-10.7b-bnb-4bit",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gemma-7b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
```

### 1-4. PEFT ëª¨ë¸ ì„¤ì •

FastLanguageModel ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LoRA ê¸°ë²•ì„ ì ìš©í•´ë³´ì. ê°ê°ì˜ ì˜µì…˜ì— ëŒ€í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- r : RoRAì˜ ë­í¬ë¥¼ ì„¤ì •í•œë‹¤. LoRAëŠ” ì €ë­í¬ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ë‹¤. ê°’ì´ ì‘ì„ ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“¤ì§€ë§Œ, ëª¨ë¸ì˜ í‘œí˜„ ëŠ¥ë ¥ì´ ì œí•œë  ìˆ˜ ìˆë‹¤.
- target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë¸ ë‚´ë¶€ì˜ íŠ¹ì • ëª¨ë“ˆì„ ì§€ì •í•´ì¤€ë‹¤. 
- lora_alpha: LoRAì˜ alpha íŒŒë¼ë¯¸í„°. 
- lora_dropout: LoRAì˜ ë“œë¡­ì•„ì›ƒ. ë“œë¡­ì•„ì›ƒì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.
- bias: ë°”ì´ì–´ìŠ¤ í•­ëª©ì˜ ì‚¬ìš©ì—¬ë¶€.  
- use_gradient_checkpointing: unsloth ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’íŒë‹¤. íŠ¹íˆ unsloth ê¸°ë²•ì€ íš¨ìœ¨ì ì¸ ê·¸ë ˆë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ê¸°ë²•ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œ ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤.
- random_state: ë‚œìˆ˜ ìƒì„± ì‹œë“œ
- use_rslora: Falseë¡œ ì§€ì •
- loftq_config: LoFtR(Low-Rank Factorized Tensor Reinforcement) ì–‘ìí™” ì„¤ì •. Noneë¡œ í•˜ì—¬ ì–‘ìí™” ì ìš©í•˜ì§€ ì•ŠìŒ. 

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, 
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",], 
    lora_alpha = 16,
    lora_dropout = 0, 
    bias = "none",    
    use_gradient_checkpointing = "unsloth",
    random_state = 2024,
    use_rslora = False, 
    loftq_config = None
)
```

### 1-5. ë°ì´í„° ì„¤ì •

ì´ì œ íŒŒì¸íŠœë‹ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì„¤ì • í•´ë³´ì. ìš°ì„  ë‹¤ìŒê³¼ ê°™ì´ AlpacaPrompt í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ì¤€ë‹¤. ë˜í•œ EOS í† í°ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ìƒì„±ì´ ë¬´í•œíˆ ê³„ì†ë˜ì§€ ì•Šë„ë¡ ì„¤ì • í•´ì£¼ì. 



```python
alpaca_prompt = """Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.

### 1-Instruction:
{}

### 1-Input:
{}

### 1-Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

```

### 1-6. ë°ì´í„° ì¤€ë¹„

ì´ì œ ë°ì´í„°ë¥¼ ì¤€ë¹„ í•´ë³´ì. ë°ì´í„°ëŠ” 'instruction', 'output', 'input' í˜•ì‹ìœ¼ë¡œ ì§€ì •í•´ì£¼ê³ , ì•ì„œ ë§Œë“  formatting_prompts_func() í•¨ìˆ˜ë¥¼ í†µí•´ ë³€í™˜ í•´ì£¼ì—ˆë‹¤. **ì°¸ê³ ë¡œ ì´ ë°ì´í„°ëŠ” Geminië¡œ ë§Œë“  ë°ì´í„°ì´ë‹¤.** ì´ì œ ì¡°ë§Œê°„ ë²•ë¥ ë°ì´í„°ë¡œ ì˜ ë§Œë“¤ì–´ì„œ ìì„¸í•˜ê²Œ íŒŒì¸íŠœë‹ í•´ë³¼ ì˜ˆì •

- [uiyong/gemini_result_kospi_0517_22](https://huggingface.co/datasets/uiyong/gemini_result_kospi_0517_22)

<center>
![](images/ë°ì´í„°ì…‹0617.PNG){width=70%}
</center>

```python
# ì£¼ê°€ ì¦ê¶Œ ë³´ê³ ì„œ gemini ë°ì´í„°ì…‹
owen_dataset = "uiyong/gemini_result_kospi_0517_22"

owen_dataset = load_dataset(owen_dataset, split="train")

dataset = owen_dataset.map(formatting_prompts_func, batched = True)

dataset[2]
```
```
{'instruction': '2024-05-17ì— ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì˜ ì£¼ê°€ ë³´ê³ ì„œëŠ” ì–´ë–¤ê°€ìš”?',
 'output': '**ì¼ì:** 2024ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤\n\n**ë“±ë½ë¥ :**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 2024ë…„ 5ì›” 17ì¼ í˜„ì¬, 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„ ì„ ìƒí–¥ ëŒíŒŒí•˜ì—¬ ê³¨ë“ í¬ë¡œìŠ¤ë¥¼ í˜•ì„±í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ë¥¼ ë‚˜íƒ€ëƒ„.\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸:**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„  ê³¨ë“ í¬ë¡œìŠ¤ ì‹ í˜¸ë¡œ ì¸í•´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.\n* ì œì¡°ì—… ì§€ìˆ˜ ë˜í•œ ì •ë ¬ëœ ë°°ì—´ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ì–´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.',
 'input': '',
 'text': 'Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.\n\n### 1-Instruction:\n2024-05-17ì— ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì˜ ì£¼ê°€ ë³´ê³ ì„œëŠ” ì–´ë–¤ê°€ìš”?\n\n### 1-Input:\n\n\n### 1-Response:\n**ì¼ì:** 2024ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤\n\n**ë“±ë½ë¥ :**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 2024ë…„ 5ì›” 17ì¼ í˜„ì¬, 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„ ì„ ìƒí–¥ ëŒíŒŒí•˜ì—¬ ê³¨ë“ í¬ë¡œìŠ¤ë¥¼ í˜•ì„±í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ë¥¼ ë‚˜íƒ€ëƒ„.\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸:**\n\n* ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 5ì¼ ë° 20ì¼ ì´ë™í‰ê· ì„  ê³¨ë“ í¬ë¡œìŠ¤ ì‹ í˜¸ë¡œ ì¸í•´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.\n* ì œì¡°ì—… ì§€ìˆ˜ ë˜í•œ ì •ë ¬ëœ ë°°ì—´ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ì–´ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ë°œìƒí•¨.<eos>'}
```

### 1-7. í•™ìŠµ ëª¨ë¸ ì„¤ì •

- per_device_train_batch_size=4: ê° GPU ë˜ëŠ” CPUì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” 2ë¡œ ì„¤ì •ì„ í•´ì£¼ì—ˆë‹¤. ë•Œë¬¸ì—, ê° ë””ë°”ì´ìŠ¤ì—ì„œ í•œ ë²ˆì— 2ê°œì˜ ìƒ˜í”Œì„ ì²˜ë¦¬í•œë‹¤. (ê¸°ë³¸ê°’ì€ 8ì´ë‹¤.)
- gradient_accumulation_steps=4:  ì—¬ëŸ¬ ë°°ì¹˜ì—ì„œ ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ì‹¤ì œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•  ë¹ˆë„ë¥¼ ì§€ì •í•œë‹¤. ì´ëŠ” GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ ìœ ìš©í•˜ë‹¤. (ê¸°ë³¸ê°’ì€ 1ì´ë‹¤.)
- warmup_steps: 
- num_train_epochs=3: ì „ì²´ í•™ìŠµ ë°ì´í„° ì…‹ ë°˜ë³µíšŸìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. (ê¸°ë³¸ê°’ì€ 3ì´ë‹¤.)
- max_steps=100: ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ë¥¼ ì§€ì •í•œë‹¤. ì°¸ê³ ë¡œ, -1ë¡œ ì„¤ì • í•˜ë©´ num_train_epochsë™ì•ˆë§Œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
- logging_steps=20: 20ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì €ì¥í•œë‹¤. (ê¸°ë³¸ê°’ì€ 500ì´ë‹¤.)
- learning_rate=2e-4: í•™ìŠµë¥ ì„ ì„¤ì •í•œë‹¤. í•™ìŠµë¥ ì€ ëª¨ë¸ì´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ì†ë„ë¥¼ ê²°ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” 0.0002ë¡œ ì„¤ì • í•´ì£¼ì—ˆë‹¤. (ê¸°ë³¸ê°’ì€ 5e-5ì´ë‹¤.)
- fp16=not torch.cuda.is_bf16_supported(): GPUê°€ bf16ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° fp16ì„ ì‚¬ìš©í•œë‹¤.
- bf16=torch.cuda.is_bf16_supported(): GPUê°€ bf16ì„ ì§€ì›í•˜ëŠ” ê²½ìš° bf16 ì„ ì‚¬ìš©í•œë‹¤.
- optim="adamw_8bit": ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €ë¥¼ ì§€ì •í•œë‹¤. 8ë¹„íŠ¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • í•´ì¤€ë‹¤. (ê¸°ë³¸ê°’ì€ adamw_hfì´ë‹¤.)
- weight_decay=0.001: ê°€ì¤‘ì¹˜ ê°ì†Œ ê³„ìˆ˜ë¥¼ ì„¤ì • í•´ì¤€ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì •ê·œí™” ê¸°ë²•ì´ë‹¤. (ê¸°ë³¸ê°’ì€ 0ì´ë‹¤.)
- lr_scheduler_type="cosine": í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ ìœ í˜•ì„ ì„¤ì •í•œë‹¤. ì½”ì‚¬ì¸ í•¨ìˆ˜ ê¸°ë°˜ì˜ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ ì‹œí‚¨ë‹¤.
  - ì°¸ê³ ë¡œ constantì˜ ê²½ìš°ì—ëŠ” ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ê²Œ ì„¤ì • ëœë‹¤.
- seed=123: ì‹œë“œë²ˆí˜¸
- output_dir="outputs": í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•œë‹¤. ì—¬ê¸°ì— ëª¨ë¸ ê°€ì¤‘ì¹˜, ë¡œê·¸, ì²´í¬í¬ì¸íŠ¸ ë“±ì´ ì €ì¥ëœë‹¤.

```python
training_params = TrainingArguments(
        per_device_train_batch_size=2,  
        gradient_accumulation_steps=4, 
        warmup_steps=5,
        num_train_epochs=3,  
        max_steps=100, 
        logging_steps=20, 
        learning_rate=2e-4, 
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(), 
        optim="adamw_8bit", 
        weight_decay=0.01, 
        lr_scheduler_type="cosine", 
        seed=123, 
        output_dir="outputs",  
    )
```


### 1-8. ëª¨ë¸ í•™ìŠµ

ì´ì œ ëª¨ë¸ì„ í•™ìŠµ ì‹œì¼œë³´ì. trl ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ SFTTrainerí´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ trainer ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì. trainer.train()ëŠ” ì§ì „ì— ì •ì˜í–ˆë˜ TrainingArgumentsì™€ í•¨ê»˜ ì„¤ì •ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.

```python
tokenizer.padding_side = "right"  

trainer = SFTTrainer(
    model=model, 
    tokenizer=tokenizer, 
    train_dataset=dataset,  
    dataset_text_field="text",  
    max_seq_length=max_seq_length,  
    dataset_num_proc=2,  
    packing=False,  
    args=training_params
)
```
```python
trainer_stats = trainer.train()
```
```
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 230 | Num Epochs = 4
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 4
\        /    Total batch size = 8 | Total steps = 100
 "-____-"     Number of trainable parameters = 50,003,968
 [100/100 02:42, Epoch 3/4]
Step	Training Loss
20	1.215200
40	0.513700
60	0.430400
80	0.326300
100	0.271400
```


### 1-9. ëª¨ë¸ í…ŒìŠ¤íŠ¸

ì´ì œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ í•´ë³´ì. 

```python
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "í•˜ì´ë‹‰ìŠ¤ì˜ 5ì›” 17ì¼ ì¦ê¶Œ í˜„í™©ì€ ì–´ë–¤ê°€ìš”?", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)
tokenizer.batch_decode(outputs)
```
```
['<bos>Below is an instruction that describes a taskWrite a response that appropriately completes the request in korean language.\n\n### 1-Instruction:\ní•˜ì´ë‹‰ìŠ¤ì˜ 5ì›” 17ì¼ ì¦ê¶Œ í˜„í™©ì€ ì–´ë–¤ê°€ìš”?\n\n### 1-Input:\n1, 1, 2, 3, 5, 8\n\n### 1-Response:\n**ì¦ê¶Œ ë³´ê³ ì„œ**\n\n**ì¼ì:** 2022ë…„ 5ì›” 17ì¼\n\n**ì¢…ëª©:** í•˜ì´ë‹‰ìŠ¤\n\n**ë“±ë½ë¥ **\n\n2022ë…„ 5ì›” 17ì¼ ê¸°ì¤€ í•˜ì´ë‹‰ìŠ¤ì˜ ë“±ë½ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n* **ì „ì¼ ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n* **20ì¼ ì´ë™í‰ê· ì„  ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n* **60ì¼ ì´ë™í‰ê· ì„  ëŒ€ë¹„:** í•˜í–¥ íšŒê·€\n\n**ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸**\n\n* **í•˜í–¥ íšŒê·€:** ë§¤ë„ ì‹ í˜¸\n* **ì •ë°°ì—´:** ë§¤ìˆ˜ ì‹ í˜¸<eos>']
```


### 1-10. huggingfaceì— GGUF ì—…ë¡œë“œ

ì´ì œ ì´ ëª¨ë¸ì„ HuggingFaceì— ì—…ë¡œë“œ í•´ë³´ì. ì´ì œë¶€í„°ëŠ” ì•„ì£¼ ê°„ë‹¨í•œë‹¤. ë¯¸ë¦¬ hugginfaceì— ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ê³  ì—…ë¡œë“œí•˜ë©´ ë.

```python
gguf_model_nm = 'stock-report-gguf'

# Quantization ë°©ì‹ ì„¤ì •
quantization_method = "q8_0"

# Hub ì— ì—…ë¡œë“œ
model.push_to_hub_gguf(
    gguf_model_nm,
    tokenizer,
    quantization_method=quantization_method,
    token='huggingface í† í°',
)
```

<center>
![](images/gguf_ëª¨ë¸.PNG){width=80%}
</center>

## 2. Ollama gguf íŒŒì¼ í…ŒìŠ¤íŠ¸

ì´ì œ ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  Ollamaì— ì˜¬ë ¤ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³´ì. ìš°ì„  Ollamaê°€ ìˆëŠ” ì„œë²„ë¥¼ í•˜ë‚˜ ì¤€ë¹„ í•´ë‘ì. ë˜ë„ë¡ GPUê°€ ìˆëŠ” ì„œë²„ê°€ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, AWSì˜ 'g4dn.xlarge' ì¸ìŠ¤í„´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³¸ë‹¤. 'g4dn.xlarge'ëŠ” ì°¸ê³ ë¡œ T4 GPU ì´ë©°, AWSì™€ GPUì„œë²„ëŠ” ë‚˜ì¤‘ì— ì •ë¦¬ë¥¼ í•´ë³¼ ì˜ˆì •ì´ë‹¤. Ollama ì„¤ì¹˜ê°€ ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ë‹¤ìŒì˜ ë§í¬ë¥¼ ì°¸ê³  í•˜ì. 

- [Ollamaì— ëŒ€í•´ ì•Œì•„ë³´ì](https://unfinishedgod.netlify.app/2024/04/26/llm-ollama-1/)

### 2-1. GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ 

ìš°ì„  GPUì„œë²„ê°€ ìˆëŠ” ê³³ìœ¼ë¡œ ê°€ì„œ í´ë”ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì£¼ì. gguf_testë¡œ ë§Œë“¤ì—ˆìœ¼ë©° ê²½ë¡œë¥¼ gguf_testë¡œ ì´ë™ í•´ì¤€ë‹¤. 

```bash
$ mkdir gguf_test
$ cd gguf_test 
```

ê·¸ë¦¬ê³  ì´ì œ ë‹¤ìŒê³¼ ê°™ì´ HuggingFaceì—ë” GGUF íŒŒì¼ì„ í´ë¦­ í•´ì¤€ë‹¤. 

<center>
![](images/hf_gguf_2.png){width=80%}
</center>

ê·¸ë¦¬ê³  download íŒŒì¼ì„ ìš°í´ë¦­í•˜ì—¬ ë§í¬ ì£¼ì†Œë¥¼ ë³µì‚¬ í•´ì¤€ë‹¤.

<center>
![](images/hf_gguf_3 (1).png){width=80%}
</center>

ê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ wget ì»¤ë§¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ì„ ì§„í–‰ í•´ì¤€ë‹¤.

```bash
$ wget https://huggingface.co/uiyong/stock-report-gguf/resolve/main/stock-report-gguf-unsloth.Q8_0.gguf
```

### 2-2. Modelfile ìƒì„±

ì´ì œ ì´ gguf íŒŒì¼ì„ ollamaì— ë³¸ê²©ì ìœ¼ë¡œ ì˜¬ë¦´ ì˜ˆì •ì¸ë°, Modelfileì„ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ íŒŒì¼ì„ ë§Œë“¤ì–´ ì£¼ì.

```bash
$ touch Modelfile
```

ê·¸ë¦¬ê³  ì´ Modelfileì„ ì„¤ì •í•˜ê¸° ìœ„í•´ vi í¸ì§‘ê¸°ë¡œ ë„˜ì–´ê°€ì.

```bash
$ vi Modelfile
```

ê·¸ë¦¬ê³  ì´ì œ ì´ Modelfileì— gguf íŒŒì¼ê³¼ ì±—ë´‡ì— í•„ìš”í•œ ì„¤ì •ì„ ì§„í–‰í•´ì£¼ë©´ ëœë‹¤. 

```
FROM stock-report-gguf-unsloth.Q8_0.gguf

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER temperature 0
PARAMETER num_predict 3000
PARAMETER num_ctx 4096
PARAMETER stop <s>
PARAMETER stop </s>
```

### 2-3. Ollamaì— ëª¨ë¸ ë“±ë¡

Modelfileì— ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„± í–ˆìœ¼ë©´ ì´ì œ Ollamaì— ëª¨ë¸ì„ ë“±ë¡í•´ì£¼ì. ì»¤ë§¨ë“œ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

- `ollama create (ëª¨ë¸ëª…) -f Modelfile`

```python
$ ollama create stock-report-Q8_9 -f Modelfile
```

ì„±ê³µì ìœ¼ë¡œ ì˜¬ë¼ê°”ìœ¼ë©´ `ollama list` ì»¤ë§¨ë“œë¥¼ í†µí•´ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤.

```bash
$ ollama list
```
```
NAME                            ID              SIZE    MODIFIED
stock-report-Q8_9:latest        59aaf0d0db3e    9.1 GB  7 minutes ago
```

### 2-4. LLM ì‹¤í–‰

ê·¸ë™ì•ˆ íŒŒì¸íŠœë‹ í–ˆë˜ ëª¨ë¸ì„ ì´ì œ ì‹¤í–‰í•´ë³´ì. ollamaì—ì„œ llmì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì»¤ë§¨ë“œ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

- `ollama run (ëª¨ë¸ëª…):(íƒœê·¸ëª…)`

```bash
$ ollama run stock-report-Q8_9:latest 
```

### 2-5. LLM í…ŒìŠ¤íŠ¸ 
 
ì´ì œ ê·¸ë™ì•ˆ íŒŒì¸íŠœë‹ í–ˆë˜ ê²ƒì„ ë“œë””ì–´ Ollamaì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ë³´ì. 

```
>>> í•˜ì´ë‹‰ìŠ¤ì˜ ì¦ê¶Œë³´ê³ ì„œ ì•Œë ¤ì¤˜
**ì¦ ëŸ‰:**

2024ë…„ì— í•˜ ì´ë‹ˆí¬ìŠ¤ëŠ” **ì •ë°°ì—´(ë§¤ìˆ˜)** ì‹ í˜¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ë§¤ë ¥ì ì¸ íˆ¬ì ì„±ê³¼ ë¥¼
 ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></div><br/>
```

ìƒë‹¹íˆ ë§ì€ ê³ ë„í™”ê°€ í•„ìš”í•´ ë³´ì´ì§€ë§Œ ê·¸ë˜ë„ ìš°ì„  íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ ë¡œì»¬ PCì—ì„œ ì‹¤í–‰ì´ ë˜ì—ˆë‹¤ëŠ” ì ì— ë§Œì¡±í•˜ê³  ë§ˆë¬´ë¦¬ í•˜ì.

## ì´í‰

ChatGPTê°€ ë‚˜ì˜¤ê³  LLama2ê°€ ë‚˜ì˜¤ê³  LLMì‹œì¥ì´ í™œë°œí•´ ì§€ë©´ì„œ ê´€ì‹¬ì„ ê°–ê²Œ ë˜ì—ˆë‹¤. ì²˜ìŒì—” ChatGPT APIê¸°ë°˜ì˜ ë­ì²´ì¸ì„ ì‚¬ìš©í•œ RAGë¡œ ì‹œì‘í–ˆì—ˆë‹¤. ë‹¹ì‹œì—ëŠ” ì•„ë¬´ê²ƒë„ ëª¨ë¥´ê³  ë§ì€ ì–‘ì˜ ë²•ë¥  ë°ì´í„°ë¥¼ RAGë¡œ ì²˜ë¦¬ í•´ë³´ë ¤ë‹ˆ ìƒë‹¹íˆ ë§ì€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªì—ˆê³  íŒŒì¸íŠœë‹ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´í›„ì— ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•œ íŒŒì¸íŠœë‹ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìœ¼ë©°, ê²°êµ­ì—” ì´ ëª¨ë¸ì„ Ollamaì—ì„œ ì‘ë™í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ì œ íŒŒì¸íŠœë‹ ê³ ë„í™”ì™€ ollamaê°€ ì•„ë‹Œ vllmì„ ì‚¬ìš©í•˜ëŠ” ê³¼ì • ë“±ë“±ë„ í•˜ë‚˜ì”© ë¸”ë¡œê·¸ í•˜ë©´ë” ë‚˜ì•„ê°€ë©´ ë ë“¯.


---

## Reference 

- [ì •ìš°ì¼ë‹˜ ë¸”ë¡œê·¸ - LLM ëª¨ë¸ ì €ì¥ í˜•ì‹ GGML, GGUF](https://wooiljeong.github.io/ml/ggml-gguf/)
- [ì •ìš°ì¼ë‹˜ ë¸”ë¡œê·¸ - GGUF íŒŒì¼ë¡œ ë¡œì»¬ì—ì„œ LLM ì‹¤í–‰í•˜ê¸°](https://wooiljeong.github.io/ml/gguf-llm/)
- [HK CODEë‹˜ ìœ íŠœë¸Œ - llama3_ë°ì´í„°ìƒì„±_íŒŒì¸íŠœë‹_finetuning_gguf_ollama_rag](https://www.youtube.com/watch?v=RrNX04J4r1Y&t=16s)
- [í…Œë””ë‹˜ ìœ íŠœë¸Œ - ë¬¸ì„œ ê¸°ë°˜ QA ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ğŸ¤– ì§„í–‰ í›„ ë¡œì»¬ì—ì„œ ëª¨ë¸ ì¶”ë¡ í•˜ê¸°](https://www.youtube.com/watch?v=oZY0D8N6bC8)
- [í…Œë””ë‹˜ ê¹ƒí—™ - 02_Unsloth_llama3_íŒŒì¸íŠœë‹_alpaca](https://github.com/teddylee777/langchain-kr/blob/main/15-FineTuning/02_Unsloth_llama3_íŒŒì¸íŠœë‹_alpaca.ipynb)
- [unslothai-ê³µì‹ github](https://github.com/unslothai/unsloth)



